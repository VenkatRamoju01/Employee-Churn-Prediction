{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "401PgchTPr4E"
   },
   "source": [
    "## Execute: Intepret model & share insights with stakeholders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex8pgn5iNzau"
   },
   "source": [
    "## Recall evaluation metrics\n",
    "\n",
    "- **AUC** is the area under the ROC curve; it's also considered the probability that the model ranks a random positive example more highly than a random negative example. \n",
    "- **Precision** measures the proportion of data points predicted as True that are actually True, in other words, the proportion of positive predictions that are true positives.\n",
    "- **Recall** measures the proportion of data points that are predicted as True, out of all the data points that are actually True. In other words, it measures the proportion of positives that are correctly classified.\n",
    "- **Accuracy** measures the proportion of data points that are correctly classified.\n",
    "- **F1-score** is an aggregation of precision and recall.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXrsxT498Z7h"
   },
   "source": [
    "### Summary of model results\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "The logistic regression model achieved precision of 80%, recall of 83%, f1-score of 80% (all weighted averages), and accuracy of 83%, on the test set.\n",
    "\n",
    "**Tree-based Machine Learning**\n",
    "\n",
    "After conducting feature engineering, the decision tree model achieved AUC of 93.8%, precision of 87.0%, recall of 90.4%, f1-score of 88.7%, and accuracy of 96.2%, on the test set. The random forest modestly outperformed the decision tree model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MOMqelNLn2v"
   },
   "source": [
    "### Conclusion, Recommendations, Next Steps\n",
    "\n",
    "The models and the feature importances extracted from the models confirm that employees at the company are overworked. \n",
    "\n",
    "To retain employees, the following recommendations to be implemented:\n",
    "\n",
    "* Cap the number of projects that employees can work on.\n",
    "* Consider promoting employees who have been with the company for atleast four years, or conduct further investigation about why four-year tenured employees are so dissatisfied. \n",
    "* Either reward employees for working longer hours, or don't require them to do so. \n",
    "* If employees aren't familiar with the company's overtime pay policies, inform them about this. If the expectations around workload and time off aren't explicit, make them clear. \n",
    "* Hold company-wide and within-team discussions to understand and address the company work culture, across the board and in specific contexts. \n",
    "* High evaluation scores should not be reserved for employees who work 200+ hours per month. Consider a proportionate scale for rewarding employees who contribute more/put in more effort. \n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "It may be justified to still have some concern about data leakage. It could be prudent to consider how predictions change when `last_evaluation` is removed from the data. It's possible that evaluations aren't performed very frequently, in which case it would be useful to be able to predict employee retention without this feature. It's also possible that the evaluation score determines whether an employee leaves or stays, in which case it could be useful to pivot and try to predict performance score. The same could be said for satisfaction score. \n",
    "\n",
    "We can also try building a K-means model on this data to analyze the clusters. This may yield valuable insight as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
